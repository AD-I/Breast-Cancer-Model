{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparseCategoricalCrossentropy.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "FzNWVEyUez9v"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AD-I/Breast-Cancer-Model/blob/master/Breast%20Cancer%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESN8bzEL-h-g",
        "colab_type": "text"
      },
      "source": [
        "# Breast Cancer Neuronal Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-bbZWdT4wmK",
        "colab_type": "text"
      },
      "source": [
        "This notebook is about Deep Neuroanl Network facing with the classification of Breast Cancer between malignant and benign. With supervised learning and using the data from Kaggle [Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data) the network will finally get an accuaracy about 96%. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8XBHaRs-WLk",
        "colab_type": "text"
      },
      "source": [
        "## Import needed libraries \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVjo59QMStbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgDSHqhl-uW-",
        "colab_type": "text"
      },
      "source": [
        "## Load breast cancer dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBIgsL-0tesI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/AD-I/Breast-Cancer-Model.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNiVN_FGtuQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('Breast-Cancer-Model/data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqawCEF_GRAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm66e3SF7Lz0",
        "colab_type": "text"
      },
      "source": [
        "As we can see, diagnosis column will be the target while the otherones will be the features.\n",
        "\n",
        "Also, diagnosis column must be transformed to numerical data (malignant: 1, benign: 0) and the final column shouldn't be there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRNKTfZy-2nv",
        "colab_type": "text"
      },
      "source": [
        "##Modify the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPTJPJzIGwh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mapdict = {'M':1,'B':0}\n",
        "df['diagnosis'] = df['diagnosis'].map(mapdict)\n",
        "df = df.drop(columns=['Unnamed: 32'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmOORgIWG3wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd8LH4LEGUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmRGs7pv8IJE",
        "colab_type": "text"
      },
      "source": [
        "Let's check the features that most influence the final diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omDLWgfcGSQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.corr()['diagnosis'].sort_values(ascending=[False])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2blzsSbI8ikQ",
        "colab_type": "text"
      },
      "source": [
        "In my case, I will take the first 12 cause they are the most influential features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU5cJ1X4_Icc",
        "colab_type": "text"
      },
      "source": [
        "##Extract the data by **Features** and **Labels** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx_NI-PjTC_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = df['diagnosis']\n",
        "my_features = ['concave points_worst',\n",
        "               'perimeter_worst',\n",
        "               'concave points_mean',\n",
        "               'radius_worst',\n",
        "               'perimeter_mean',\n",
        "               'area_worst',\n",
        "               'radius_mean',\n",
        "               'area_mean',\n",
        "               'concavity_mean',\n",
        "               'concavity_worst',\n",
        "               'compactness_mean',\n",
        "               'compactness_worst'\n",
        "               ]\n",
        "features = df[my_features]\n",
        "labels = labels.to_numpy()\n",
        "features = features.to_numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZPth5ZB9Aeg",
        "colab_type": "text"
      },
      "source": [
        "Let's check the data shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaZtRmMMVEA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Labels')\n",
        "print(labels.shape, end=' ')\n",
        "print(labels.ndim)\n",
        "\n",
        "print('\\nFeatures:')\n",
        "print(features.shape, end=' ')\n",
        "print(features.ndim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMK5fBJ_PiU",
        "colab_type": "text"
      },
      "source": [
        "###Split the data between train, validation and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI3YROv89G55",
        "colab_type": "text"
      },
      "source": [
        "Before the split, I scale the features to improve training performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaCI04Sv0LGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "min_max_scaler = MinMaxScaler()\n",
        "features = min_max_scaler.fit_transform(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0mVxMQS9-T3",
        "colab_type": "text"
      },
      "source": [
        "Now it's time to split the data in:\n",
        "* Training data: 70%\n",
        "* Validation data: 20%\n",
        "* Test data: 10%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXcHu-Rri70B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(features,\n",
        "                                                      labels,\n",
        "                                                      test_size=0.3)\n",
        "x_valid, x_test, y_valid, y_test = train_test_split(x_valid,\n",
        "                                                    y_valid,\n",
        "                                                    test_size=0.33)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViiHvgJTgce7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Train')\n",
        "print('Features: ', x_train.shape, end=' Labels: ')\n",
        "print(y_train.shape)\n",
        "print('Validation')\n",
        "print('Features: ', x_valid.shape, end=' Labels: ')\n",
        "print(y_valid.shape)\n",
        "print('Test')\n",
        "print('Features: ', x_test.shape, end=' Labels: ')\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGvcwCfH-ePJ",
        "colab_type": "text"
      },
      "source": [
        "Finally transform numpy arrays to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib9tJFsLcEma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = tf.convert_to_tensor(x_train, np.float64)\n",
        "y_train = tf.convert_to_tensor(y_train, np.float64)\n",
        "\n",
        "x_valid = tf.convert_to_tensor(x_valid, np.float64)\n",
        "y_valid = tf.convert_to_tensor(y_valid, np.float64)\n",
        "\n",
        "x_test = tf.convert_to_tensor(x_test, np.float64)\n",
        "y_test = tf.convert_to_tensor(y_test, np.float64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqtO8GfTAN4D",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiPbgqm39ZM4",
        "colab_type": "text"
      },
      "source": [
        "### Functional API Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMgc5Ch9f95y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(lr=0.001, n_hidden_nodes=6, dropout=True):\n",
        "  '''Creates a model by the Keras functional API. Layers:\n",
        "  Input: 12 relu, Dense: n_hidden_nodes relu, Output: 2 softmax.\n",
        "  lr = (Float) learning rate for the optimizer Adam.\n",
        "  n_hidden_nodes = (Int) N nodes for the hidden layer.\n",
        "  droput = (Bool) If True, it will be added a dropout layer between Dense16\n",
        "           and output.\n",
        "  return's a tf.keras.Model\n",
        "  '''\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  inputs = tf.keras.Input(shape=(12,), name='input')\n",
        "  x = tf.keras.layers.Dense(n_hidden_nodes, activation='relu')(inputs)\n",
        "  if dropout:\n",
        "    drop = tf.keras.layers.Dropout(.2)(x)\n",
        "    outputs = tf.keras.layers.Dense(2, activation='softmax')(drop)\n",
        "  else:\n",
        "    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "\n",
        "  model = tf.keras.Model(\n",
        "      inputs=inputs, outputs=outputs, name='breast_cancer_model')\n",
        "\n",
        "  model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HydiXMVZ-3nV",
        "colab_type": "text"
      },
      "source": [
        "First of all, check the appropriate learning rate for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UciuUzQmgzQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(1e-5, 16, dropout=False)\n",
        "epochs = 150\n",
        "batch_size = 32\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
        "    (lambda epoch: 1e-5 * 10**(epoch / 30))\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    callbacks=[lr_schedule]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6SuSIfK-22D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HQJ-VhK_A1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model, to_file='model_graph.png', show_shapes=True, show_layer_names=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WsC6cfm_bkD",
        "colab_type": "text"
      },
      "source": [
        "Plot the curve between the learning rate and loss. Finally I toke the value of 2e-3 for the learning rate, just before it becomes unstable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9oqMS9KjGCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.semilogx(history.history['lr'], history.history['loss'])\n",
        "plt.axis([1e-5, 1e-1, 0, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpAxc5QT_0Tf",
        "colab_type": "text"
      },
      "source": [
        "Now train the model with the good learning rate. Also added early stopping and model checkpoint to find the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjaLscSvweqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(2e-3, 16)\n",
        "epochs = 600\n",
        "batch_size = 32\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=15)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'model_checkpoint.h5', save_best_only=True)\n",
        "\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        "    validation_data=(x_valid, y_valid),\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bez9CRiQAVsg",
        "colab_type": "text"
      },
      "source": [
        "Finally load the best model between this epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv86DW_M3RAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model('model_checkpoint.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERMbcBlTAoE5",
        "colab_type": "text"
      },
      "source": [
        "###Show loss and accuaracy per epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnBEioJ7XMPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs_range = range(len(acc))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUbcv-JYAvJB",
        "colab_type": "text"
      },
      "source": [
        "###Predict values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsuBJAPNBHzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_value(model, features, labels, n):\n",
        "  '''Predict some value\n",
        "  model: tf.keras.Model trained\n",
        "  features: \n",
        "  labels: \n",
        "  n: (int) index of feautres\n",
        "  return: \n",
        "  '''\n",
        "  data = tf.convert_to_tensor(features[n])\n",
        "  data = tf.expand_dims(data, axis=0)\n",
        "  print(f\"Label: {labels[n]}\")\n",
        "  return model.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQHvN7bupzYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_value(model, features, labels, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StNIVwB5aQ8h",
        "colab_type": "text"
      },
      "source": [
        "### Use test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAtQONUcg2lK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Total values: ', len(tf.keras.backend.get_value(x_test)))\n",
        "for test_feature, test_label in zip(x_test, y_test):\n",
        "  true_label = tf.keras.backend.get_value(test_label)\n",
        "  print(f'Label: {true_label}', end=\" \")\n",
        "\n",
        "  test_feature = tf.expand_dims(test_feature, 0)\n",
        "  predicted = model.predict(test_feature)[0]\n",
        "  \n",
        "  if predicted[0] > predicted[1]:\n",
        "    if true_label == 0.0:\n",
        "      # True Negative\n",
        "      print(predicted, \"\\U0000274E\" + \"\\n\")\n",
        "    else:\n",
        "      # False Negative\n",
        "      print(predicted, \"\\U0000274C\" + \"\\n\")\n",
        "  else:\n",
        "    if true_label == 1.0:\n",
        "      # True Positive\n",
        "      print(predicted, \"\\U00002705\" + \"\\n\")\n",
        "    else:\n",
        "      # False Positive\n",
        "      print(predicted, \"\\U0000274C\" + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq5PFENCetjg",
        "colab_type": "text"
      },
      "source": [
        "## Save trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoqYEL7kYWcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.saved_model.save(model, './modelTF')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vGbqgLf4pR",
        "colab_type": "text"
      },
      "source": [
        "Download the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUdy-ax8b9Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r model.zip ./modelTF\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRh5LRmycGS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('./model.zip')\n",
        "except ImportError:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}